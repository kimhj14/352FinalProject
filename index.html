<!doctype html>
<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>EECS 352 Final Project</title>
<link rel="stylesheet" href="http://dhbhdrzi4tiry.cloudfront.net/cdn/sites/foundation.min.css">
<link rel="stylesheet" href="css/main.css">
<link href='http://cdnjs.cloudflare.com/ajax/libs/foundicons/3.0.0/foundation-icons.css' rel='stylesheet' type='text/css'>

</head>
<body>

<div class="top-bar">
	<ul class="menu">
		<li class="menu-text">EECS 352 Final Project</li>
		<li><a href="#overview">Overview</a></li>
		<li><a href="#approach">Approach</a></li>
		<li><a href="#results">Results</a></li>
		<li><a href="#conclusion">Conclusion</a></li>
		<li><a href="#references">References</a></li>
	</ul>
</div>

<div class="callout large">
	<div class="row column text-center">
		<h1>Instrument Recognition in Monophonic Melodies</h1>
		<p class="lead">~ Final project for Northwestern University <a href="http://cs.northwestern.edu/~pardo/courses/eecs352/">EECS 352</a>, Professor Bryan Pardo ~</p>
		<br>
		<div class="row">
			<div class="medium-4 columns">
				<h4>Aleka Cheung</h4>
				<p>alekacheung2017@u.northwestern.edu</p>
			</div>
			<div class="medium-4 columns">
				<h4>Phoebe Kim</h4>
				<p>heekim2016@u.northwestern.edu</p>
			</div>
			<div class="medium-4 columns">
				<h4>Michael Wang</h4>
				<p>michaelwang2016@u.northwestern.edu</p>
			</div>
		</div>
	</div>
</div>

<div id="overview">
	<div class="row">
		<div class="small-11 small-centered medium-11 medium-centered large-11 large-centered columns">
			<h2>Overview</h2>
			<p>Our project is to create a system that could analyze and identify which instrument (violin, trumpet, or vibraphone) is playing a tone. Our system will try to complete this task by using Mel-frequency cepstral coefficients (MFCCs). </p>
			<br>
			<h3><u>Motivation</u></h3>
			<p>It could be a useful system because successfully developing a musical instrument identification system (MIIS) will help us learn how machines classify sounds, and give us some insight as to how ML can be applied to the field of signal processing. A successful MIIS could also make it possible to create a database of songs by instrument type. In addition, MIIS could enhance the current transcription systems to be able to indicate which lines transcribed are played by which instruments to make the transcribed scores even more informative.</p>
		</div>
	</div>
</div>

<hr>

<div id="approach">
	<div class="row">
		<div class="small-11 small-centered medium-11 medium-centered large-11 large-centered columns">
			<h2>Our Approach</h2>
			<p>Our system will try to determine whether a melody is being played by a violin, a trumpet, or a vibraphone by using Mel-frequency cepstral coefficients (MFCCs).</p>

			<h3><u>Data Set</u></h3>
			<p>Our data set consists of 100 files with recordings of a single note from a violin, Bb trumpet, and vibraphone. These recordings are downloaded from University of Iowa Electronic Music Studios. The original format was 24-bit stereo aiff; we converted that to 16-bit mono wav with Audacity to make the files compatible with librosa. They are already labeled by instruments and note pitch.</p>

			<h3><u>Process:</u></h3>
			<p> 1. Generate an MFCC of each audio file using librosa </p>
			<p> 2. Flatten the 2D arrays into vectors and feed 70 percent of the data into an SVM (with the corresponding instrument as the categorization) </p>
      <p> 3. Normalize cepstrogram data as needed </p>
      <p> 4. Using the other 30 percent of the cepstrogram data as a test set, evaluate accuracy of system </p>
		</div>
	</div>
</div>

<hr>

<div id="results">
	<div class="row">
		<div class="small-11 small-centered medium-11 medium-centered large-11 large-centered columns">
			<h2>Results</h2>
			<p>We generated the MFCCs of each instrument and tone from the sample recordings and plotted a cepstrogram for each instrument. Then we flattened and normalized the MFCCs.</p>
			<br>
			<div class="row">
				<div class="medium-4 columns">
					<p>Trumpet</p>
					<img src="img/trumpet-cepstrogram.jpg">
				</div>
				<div class="medium-4 columns">
					<p>Vibraphone</p>
					<img src="img/vibraphone-cepstrogram.jpg">
				</div>
				<div class="medium-4 columns">
					<p>Violin</p>
					<img src="img/violin-cepstrogram.jpg">
				</div>
			</div>
			<br>
			<p>We had 36 files of Trumpet, 41 files of Vibraphone, and 23 of Violin. For every file, only the first 2.5 seconds were imported into librosa (at a sample rate of 22050) and used to generate an MFCC with window size = 2048, hop size = 1024, and 88 cepstral coefficients. These cepstrograms are flattened, normalized with librosa (to control for variations in instrument volume across samples) and dumped to a binary file format. Then, the data was divided into a training set comprising 70% of the data and a test set with the other 30% (division was stratified by instrument). The training data is fed into a Support Vector Machine (SVM), where each vector was also given a category of (0, 1, 2), corresponding to (Trumpet, Vibraphone, Violin).</p>

			<p>We then feed the other 30% of the data into the trained SVM model, and our results are such:</p>
			<li>For trumpet, we have 13/13 (100%) predicted as trumpet.</li>
			<li>For vibraphone, we have 0/12 (0%) predicted as vibraphone.</li>
			<li>For violin, we have 0/7 (0%) predicted as violin.</li>
      <br>
      <p> That was the result of our final attempt. Previous attempts tried to do similar things, for example without normalization, and also without truncating (but instead feeding the MFCCs cut by time index). Neither of those yielded good results either. Specifically, feeding the sliced MFCC data would probably not have worked in hindsight, since the cepstrum of an instrument is not independent of the time a note has been held on it. </p>
			<br>
			<p>Curiously, our model seems to overfit for trumpet every time.</p>
		</div>
	</div>
</div>

<hr>

<div id="conclusion">
	<div class="row">
		<div class="small-11 small-centered medium-11 medium-centered large-11 large-centered columns">
			<h2>Conclusion</h2>
      Our model doesn't do particularly well. Applying ML to audio samples is actually much more difficult than we imagined. I hypothesize that our samples are simply not normalized enough. There are differences, for example, in start time of the tone being played (some start immediately, while others have a delay). There are also significant differences in the length of different files. We tried to alleviate this by truncating all files to 2.5 seconds, but that did not seem to improve things. Perhaps we should also have increase our number of samples.
		</div>
	</div>
</div>

<hr>

<div id="references">
	<div class="row">
		<div class="small-11 small-centered medium-11 medium-centered large-11 large-centered columns">
			<h2>References</h2>
			<li><a href="http://sound.media.mit.edu/Papers/kdm-asa98.pdf"> “2pMU9. Musical instrument identification: A pattern-recognition approach”</a> by Keith D. Martin and Youngmoo E. Kim | MIT Media Lab Machine Listening Group | 1998 </li>
			<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.474.8056&rep=rep1&type=pdf">“The Use of Mel-frequency Cepstral Coefficients in Musical Instrument Identification”</a> by Roisin Loughran, Jacqueline Walker, Michael O’Neill, and Marion O’Farrell | University of Limerick, Ireland and University College Dublin, Ireland | 2004 </li>
			<li><a href="http://recherche.ircam.fr/anasyn/livshin/publications/livshin-dafx04-final.pdf">“Musical Instrument Identification in Continuous Recordings” </a> by Arie A. Livshin and Xavier Rodet | Analysis/Synthesis Team | Paris, France | 2004</li>
			<li><a href="http://www1.icsi.berkeley.edu/~dpwe/research/etc/icassp2000/pdf/1285_104.PDF">“Musical Instrument Recognition Using Cepstral Coefficients and Temporal Features”</a> by Antti Eronen and Anssi Klapuri | Signal Processing Lab, Tampere University of Technology, Finland</li>
			<li><a href="https://ccrma.stanford.edu/~pj97/MyWork/jinachitra_icme04.pdf">“Polyphonic Instrument Identification Using Independent Subspace Analysis” </a>by Pamornpol Jinachitra | CCRMA, Stanford University</li>
		</div>
	</div>
</div>

<hr>

<!-- footer -->
<div class="row column text-center">
	<p style="font size: 8px;"> Created by PK </p>
</div>

<!-- Script -->
<script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
<script src="http://dhbhdrzi4tiry.cloudfront.net/cdn/sites/foundation.js"></script>
<script>
      $(document).foundation();
    </script>
<script type="text/javascript" src="https://intercom.zurb.com/scripts/zcom.js"></script>

</body>
</html>
